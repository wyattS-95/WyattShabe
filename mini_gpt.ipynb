{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE4C4Ps9j1AV"
      },
      "source": [
        "# 🧠 Mini-GPT\n",
        "A minimal GPT implementation from scratch. This is designed to run on CPU with limited memory. Only requires `torch`, `numpy`, and `tqdm`."
      ],
      "id": "cE4C4Ps9j1AV"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1ziXnqlpj1AW"
      },
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "!pip -q install torch numpy tqdm"
      ],
      "id": "1ziXnqlpj1AW"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mc9rF534j1AX"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import re"
      ],
      "id": "mc9rF534j1AX"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CWrOBoL1j1AX"
      },
      "outputs": [],
      "source": [
        "# Sample tiny dataset (can be replaced later)\n",
        "data = \"\"\"\n",
        "Once upon a time, there was a small model learning big ideas.\n",
        "This model understood words and context, not just characters.\n",
        "It trained every day, dreaming of generalization.\n",
        "\"\"\"\n",
        "\n",
        "def word_tokenize(text):\n",
        "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "\n",
        "words = word_tokenize(data)\n",
        "vocab = sorted(set(words))\n",
        "vocab_size = len(vocab)\n",
        "stoi = {w: i for i, w in enumerate(vocab)}\n",
        "itos = {i: w for w, i in stoi.items()}\n",
        "\n",
        "def encode(words): return [stoi[w] for w in words]\n",
        "def decode(tokens): return ' '.join([itos[i] for i in tokens])"
      ],
      "id": "CWrOBoL1j1AX"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z9pkoVVYj1AX"
      },
      "outputs": [],
      "source": [
        "# Encode dataset and split\n",
        "data_ids = torch.tensor(encode(words), dtype=torch.long)\n",
        "block_size = 8  # number of word tokens per input sequence\n",
        "\n",
        "def get_batch(batch_size=4):\n",
        "    ix = torch.randint(len(data_ids) - block_size, (batch_size,))\n",
        "    x = torch.stack([data_ids[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data_ids[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y"
      ],
      "id": "Z9pkoVVYj1AX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Single self-attention head\n",
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, n_embed, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(256, 256)))\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) / C**0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out"
      ],
      "metadata": {
        "id": "tIQvKSMMlA0M"
      },
      "id": "tIQvKSMMlA0M",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_embed, num_heads):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // num_heads\n",
        "        self.heads = nn.ModuleList([\n",
        "            SelfAttentionHead(n_embed, head_size) for _ in range(num_heads)\n",
        "        ])\n",
        "        self.proj = nn.Linear(n_embed, n_embed)  # combine heads\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        return self.dropout(out)"
      ],
      "metadata": {
        "id": "4PTZisZpma_I"
      },
      "id": "4PTZisZpma_I",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feedforward layer\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embed, 4 * n_embed),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embed, n_embed),\n",
        "            nn.Dropout(0.1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "p6P3aO8Mnri8"
      },
      "id": "p6P3aO8Mnri8",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, n_embed, num_heads):\n",
        "        super().__init__()\n",
        "        self.sa = MultiHeadAttention(n_embed, num_heads)\n",
        "        self.ff = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))  # Self-attention + residual\n",
        "        x = x + self.ff(self.ln2(x))  # Feedforward + residual\n",
        "        return x"
      ],
      "metadata": {
        "id": "O-FFlY1MnsYS"
      },
      "id": "O-FFlY1MnsYS",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4e6YXB4rj1AY"
      },
      "outputs": [],
      "source": [
        "# Mini-GPT Model\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embed, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embed)\n",
        "        self.pos_embedding = nn.Embedding(256, n_embed)\n",
        "        self.block = TransformerBlock(n_embed, num_heads)\n",
        "        self.ln_f = nn.LayerNorm(n_embed)  # final layernorm before head\n",
        "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
        "\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "        pos_emb = self.pos_embedding(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.block(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "        return logits"
      ],
      "id": "4e6YXB4rj1AY"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ERvSFCeUj1AY"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = MiniGPT(vocab_size=vocab_size, n_embed=64, num_heads=4)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ],
      "id": "ERvSFCeUj1AY"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXKa4I01j1AY",
        "outputId": "47436922-10ae-4397-ce5c-6452253f3a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 4/2000 [00:00<00:59, 33.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss: 3.4357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|██▋       | 531/2000 [00:08<00:09, 149.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 500, Loss: 0.0827\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████▏    | 1028/2000 [00:11<00:06, 157.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1000, Loss: 0.0135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▌  | 1511/2000 [00:14<00:04, 110.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1500, Loss: 0.0273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [00:18<00:00, 108.98it/s]\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for step in tqdm(range(2000)):\n",
        "    xb, yb = get_batch()\n",
        "    logits = model(xb)\n",
        "    B, T, C = logits.shape\n",
        "    loss = F.cross_entropy(logits.view(B*T, C), yb.view(B*T))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if step % 500 == 0:\n",
        "        print(f\"Step {step}, Loss: {loss.item():.4f}\")"
      ],
      "id": "KXKa4I01j1AY"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xdGtkKZgj1AY"
      },
      "outputs": [],
      "source": [
        "# Generation function\n",
        "def generate(model, start_text, max_new_tokens=100):\n",
        "    model.eval()\n",
        "    idx = torch.tensor(encode(start_text), dtype=torch.long).unsqueeze(0)\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = model(idx[:, -block_size:])\n",
        "        next_id = torch.multinomial(F.softmax(logits[:, -1, :], dim=-1), num_samples=1)\n",
        "        idx = torch.cat((idx, next_id), dim=1)\n",
        "    return decode(idx[0].tolist())"
      ],
      "id": "xdGtkKZgj1AY"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLoepXxsj1AY",
        "outputId": "c52e68f4-651b-4501-f908-2303dc7a7045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: once upon a time there was a small model learning big ideas this model understood words and context not just characters it trained every day dreaming of generalization\n"
          ]
        }
      ],
      "source": [
        "# Generate text\n",
        "prompt = \"Once upon\"\n",
        "tokens = word_tokenize(prompt)  # Tokenize into words\n",
        "out = generate(model, tokens, max_new_tokens=26)  # Pass tokens (list of words) to generate\n",
        "\n",
        "print(\"Generated:\", out)  # Output is already decoded in generate"
      ],
      "id": "YLoepXxsj1AY"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}